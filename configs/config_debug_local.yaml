### HYDRA START
hydra:
  run:
    dir: outputs/${hydra.job.name}
  sweep:
    dir: outputs/${hydra.job.name}
    subdir: ${hydra.job.num}
  job:
    chdir: True # change working directory of run
    name: ${config.experiment_data.experiment_name}_${now:%d%m%y_%H%M%S}
### HYDRA END

# run_config:
#   exec_type: parallel # sequential
#   gpu_ids: [2,3]
#   runs_per_gpu: 5

# seeds: 
#   - 0

# sweep:
#   type: grid
#   axes:
#     - parameter: data.train_task_idx
#       vals: [0,1,2,3,4,5,6,7,8,9]

### 
config:
  experiment_data:
    entity: null # jkuiml-fsl
    project_name: erank_supervised
    experiment_tag: 'x.yy'
    experiment_name: cf10-erank-DEBUG
    experiment_dir: null
    seed: 0
    gpu_id: 0

  wandb:
    init:
      tags: # list(), used to tag wandblogger
        - ${config.experiment_data.experiment_tag}_exps
        - DEBUG
      notes: # str, used to make notes to wandblogger
      group: ${config.experiment_data.experiment_tag} # null 
      job_type: pretrain # examples: hypsearch, pretrain, eval, etc. 
          
    watch:
      log: parameters #null #all
      log_freq: 5000

  model:
    name: cnn2d
    out_channels: 64
    model_kwargs:
      image_size: 32
      in_channels: 3
      act_fn: relu
      layer_configs:
        - out_channels: ${config.model.out_channels}
          kernel_size: 3
          batch_norm: true
          stride: 1
          padding: 0
          max_pool_kernel_size: 2
        - out_channels: ${config.model.out_channels}
          kernel_size: 3
          batch_norm: true
          stride: 1
          padding: 0
          max_pool_kernel_size: 2
        - out_channels: ${config.model.out_channels}
          kernel_size: 3
          batch_norm: true
          stride: 1
          padding: 0
          max_pool_kernel_size: 2
      linear_output_units:
        - 10

  trainer:
    training_setup: supervised
    n_epochs: 300
    val_every: 1
    save_every: 50
    early_stopping_patience: 20
    batch_size: 512
    optimizer_scheduler:
      optimizer_name: adamw
      optimizer_kwargs:
        lr: 0.001
        weight_decay: 0.0
    init_model: null #/system/user/beck/pwbeck/projects/regularization/erank/res/init_model.p
    loss: crossentropy
    erank:
      type: none #random #pretraindiff
      loss_weight: 0.0
      dir_buffer: /system/user/beck/pwbeck/projects/regularization/erank/outputs/f_mnist-2.0-taskidxXXX_260622_110634/outputs
      buffer_size: 10 # number of directions stored in the buffer
      norm_directions: False
      use_abs_model_params: True
      log_normalized_erank: True
    num_workers: 4

  # data: 
  #   dataset: fashion_mnist
  #   dataset_kwargs:
  #     dataset_dir: /home/max/phd/data/mnist
  #   dataset_split:
  #     train_val_split: 0.8
  #     num_train_tasks: 13
  #     subsplit_first_n_train_tasks: 0
  #     num_subsplit_tasks: 0
  #     train_task_idx: 10

  data: 
    dataset: cifar10
    dataset_kwargs:
      dataset_dir: /home/max/phd/data/cifar

    dataset_split:
      train_val_split: 0.8
      num_train_tasks: 13
      subsplit_first_n_train_tasks: 0
      num_subsplit_tasks: 0
      train_task_idx: 10
      restrict_n_samples_train_task: -1