### HYDRA START
hydra:
  run:
    dir: outputs/${hydra.job.name}
  sweep:
    dir: outputs/${hydra.job.name}
    subdir: ${hydra.job.num}
  job:
    chdir: True # change working directory of run
    name: ${config.experiment_data.experiment_name}_${now:%d%m%y_%H%M%S}
### HYDRA END

run_config: # raptor
  exec_type: parallel # sequential
  gpu_ids: [0,1]
  runs_per_gpu: 12

seeds: 
  - 0

sweep:
  type: grid
  axes:
    - parameter: trainer.optimizer_scheduler.optimizer_kwargs.lr
      vals: [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]
    - parameter: trainer.optimizer_scheduler.optimizer_kwargs.weight_decay
      vals: [0.0, 0.01, 0.001, 0.0001] #[0.0, 0.0001, 0.001, 0.01, 0.1]
    - parameter: trainer.batch_size
      vals: [64, 128, 256, 512]
    - parameter: data.dataset_split.train_task_idx
      vals: [10] #[10, 11, 12]
### 
config:
  experiment_data:
    project_name: erank_supervised
    experiment_tag: '5.2'
    experiment_name: cf10-${config.experiment_data.experiment_tag}-fc_hypsearch-lr_${config.trainer.optimizer_scheduler.optimizer_kwargs.lr}-wd_${config.trainer.optimizer_scheduler.optimizer_kwargs.weight_decay}-batchsize${config.trainer.batch_size}-taskidx_${config.data.dataset_split.train_task_idx}
    experiment_dir: null
    seed: 0
    gpu_id: 2

  wandb:
    tags: # list(), used to tag wandblogger
      - ${config.experiment_data.experiment_tag}_exps
      - erank
      - rerun
    notes: Hyperparametersearch for fully-connected on CIFAR10. # str, used to make notes to wandblogger
    
    watch:
      log: null #all
      log_freq: 100

  model:
    name: fc
    model_kwargs:
      input_size: 3072 # 3x32x32
      hidden_sizes:
        - 512
        - 512
      output_size: 10
      flatten_input: true
      dropout: 0.2
      act_fn: relu

  trainer:
    n_epochs: 300
    val_every: 1
    save_every: 50
    early_stopping_patience: 20
    batch_size: XXX
    optimizer_scheduler:
      optimizer_name: adamw
      optimizer_kwargs:
        lr: XXX
        weight_decay: XXX
    init_model: null #/system/user/beck/pwbeck/projects/regularization/erank/res/init_model.p
    loss: crossentropy
    erank:
      type: none #random #pretraindiff
      loss_weight: 0.0
      dir_buffer: /system/user/beck/pwbeck/projects/regularization/erank/outputs/f_mnist-2.0-taskidxXXX_260622_110634/outputs
      buffer_size: 10 # number of directions stored in the buffer
      norm_directions: False
      use_abs_model_params: True
    num_workers: 4

  data: 
    dataset: cifar10
    dataset_kwargs:
      dataset_dir: /system/user/beck/pwbeck/data/cifar
    dataset_split:
      train_val_split: 0.8
      num_train_tasks: 13
      subsplit_first_n_train_tasks: 0
      num_subsplit_tasks: 0
      train_task_idx: XXX