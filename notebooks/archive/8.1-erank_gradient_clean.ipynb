{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "gpu_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_matrix = torch.normal(0,1,size=(10,1000))\n",
    "dir_matrix.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_matrix.shape, dir_matrix.device, dir_matrix.requires_grad, torch.linalg.norm(dir_matrix, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## erank function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erank_svd(matrix_A: torch.Tensor, center_matrix_A: bool=False) -> torch.Tensor:\n",
    "        \"\"\"Calculates the effective rank of a matrix.\n",
    "\n",
    "        Args:\n",
    "            matrix_A (torch.Tensor): Matrix of shape m x n. \n",
    "            center_matrix_A (bool): Center the matrix \n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Effective rank of matrix_A\n",
    "        \"\"\"\n",
    "        assert matrix_A.ndim == 2\n",
    "        _, s, _ = torch.linalg.svd(matrix_A, full_matrices=False)\n",
    "        # normalizes input s -> scale independent!\n",
    "        return torch.exp(torch.distributions.Categorical(s).entropy())\n",
    "\n",
    "def erank_pca_lr(matrix_A: torch.Tensor, center_matrix_A: bool=False) -> torch.Tensor:\n",
    "        \"\"\"Calculates the effective rank of a matrix.\n",
    "\n",
    "        Args:\n",
    "            matrix_A (torch.Tensor): Matrix of shape m x n. \n",
    "            center_matrix_A (bool): Center the matrix \n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Effective rank of matrix_A\n",
    "        \"\"\"\n",
    "        assert matrix_A.ndim == 2\n",
    "        _, s, _ = torch.pca_lowrank(matrix_A, center=center_matrix_A, niter=1, q=min(matrix_A.shape[0], matrix_A.shape[1]))\n",
    "        # s = torch.square(s) / (s.shape[0] - 1)\n",
    "        # normalizes input s -> scale independent!\n",
    "        return torch.exp(torch.distributions.Categorical(s).entropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = erank_svd(dir_matrix)\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# er.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the gradient of the erank look like?\n",
    "\n",
    "Context: During the experiments with erank regularized Reptile I encountered issues with Inf loss values due to (very likely) Inf gradients. \n",
    "These Inf gradients occured when we evaulated the erank at the origin. In the first inner iteration we subtract the base model from the current model parameters. This results to a zero vector.\n",
    "Hypothesis: \n",
    "The erank is not continuous differentiable! Especially not at the origin. Check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dir_matrix = dir_matrix / torch.linalg.norm(dir_matrix, ord=2, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.linalg.norm(dir_matrix, ord=2, dim=1, keepdim=True), torch.linalg.norm(norm_dir_matrix, ord=2, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erank_svd(dir_matrix), erank_svd(norm_dir_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How sensitive is the erank and its gradient to changes in the update vector? Specifically, the norm:\n",
    "\n",
    "Here the erank and the singular values of the matrix = cat([pretrain_diffs, [norm_vec, 0,0, ...]]) is calculated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_update_vec = 0.0\n",
    "\n",
    "delta = torch.zeros(size=(1,dir_matrix.shape[1])).to(dir_matrix)\n",
    "delta[0] = norm_update_vec\n",
    "delta.requires_grad_(True)\n",
    "matrix1 = torch.cat([delta, dir_matrix], dim=0)\n",
    "_, s, _ = torch.svd_lowrank(matrix1)\n",
    "erank_val = erank_svd(matrix1)\n",
    "erank_val.backward()\n",
    "erank_grad_norm = torch.linalg.norm(delta.grad)\n",
    "s, erank_val, erank_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_update_vec = 5.0\n",
    "\n",
    "delta = torch.zeros(size=(1,dir_matrix.shape[1])).to(dir_matrix)\n",
    "delta[0] = norm_update_vec\n",
    "delta.requires_grad_(True)\n",
    "matrix1 = torch.cat([delta, dir_matrix], dim=0)\n",
    "_, s, _ = torch.svd_lowrank(matrix1)\n",
    "erank_val = erank_svd(matrix1)\n",
    "erank_val.backward()\n",
    "erank_grad_norm = torch.linalg.norm(delta.grad)\n",
    "s, erank_val, erank_grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The erank is sensitive to length of update vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## erank plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates values for erank plots.\n",
    "# It analyzes the erank of a matrix if the norm of one vector is increased.\n",
    "\n",
    "# I use the directions matrix (i.e. concatenated models) and concatenate one vector which has only one entry. \n",
    "# This vector spans a one-dimensional space (a line). The value of this entry is increased.\n",
    "\n",
    "def erank_sv_norm_update_vector_sweep(dir_matrix, update_vec, erank_fn=erank_svd, normalize_dir_matrix: bool = False, xlim_logscale=[-10,4], n_points=30):\n",
    "    assert update_vec.shape == (1, dir_matrix.shape[1])\n",
    "    dir_matrix.requires_grad_(False)\n",
    "    erank_vals = []\n",
    "    erank_grad_norm_vals = []\n",
    "    singular_vals = []\n",
    "    vec_norms = torch.logspace(xlim_logscale[0], xlim_logscale[1], n_points)\n",
    "    for n in vec_norms:\n",
    "        delta = update_vec.to(dir_matrix)\n",
    "        delta = n*delta\n",
    "        delta.requires_grad_(True)\n",
    "        matrix1 = torch.cat([delta, dir_matrix], dim=0)\n",
    "        if normalize_dir_matrix:\n",
    "            # normalize matrix\n",
    "            matrix1 = matrix1 / torch.linalg.norm(matrix1, ord=2, dim=1, keepdim=True)\n",
    "        # calculate erank\n",
    "        erank_val = erank_fn(matrix1)\n",
    "        erank_vals.append(erank_val.item())\n",
    "        # calculate erank grad norm\n",
    "        erank_val.backward()\n",
    "        erank_grad_norm_vals.append(torch.linalg.norm(delta.grad).item())\n",
    "        # calculate singular values\n",
    "        _, s, _ = torch.svd_lowrank(matrix1)\n",
    "        singular_vals.append(s.detach().cpu().numpy())\n",
    "        ret_dict = {\n",
    "            'vec_norms': vec_norms,\n",
    "            'erank_vals': erank_vals,\n",
    "            'singular_vals': singular_vals,\n",
    "            'erank_grad_norm_vals': erank_grad_norm_vals\n",
    "        }\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(out_subspace: Dict[str, float], in_subspace: Dict[str, float], singular_values_ylim=[20,1000], erank_ylim=[0,13]):\n",
    "    fig = plt.figure(figsize=(25,14))\n",
    "    fig.suptitle('First row: direction outside subspace | Second row: direction inside subspace')\n",
    "    vec_norms = out_subspace['vec_norms']\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.plot(vec_norms, out_subspace['erank_vals'], 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('erank')\n",
    "    plt.xlabel('norm update vector')\n",
    "    plt.title('erank')\n",
    "    plt.ylim(erank_ylim)\n",
    "    \n",
    "    plt.subplot(2,3,2)\n",
    "    plt.plot(vec_norms, out_subspace['erank_grad_norm_vals'], 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('grad norm erank')\n",
    "    plt.xlabel('norm update vector')\n",
    "    plt.title('gradient norm of concatenated vector')\n",
    "    \n",
    "    plt.subplot(2,3,3)\n",
    "    plt.plot(vec_norms, out_subspace['singular_vals'], 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('singular value')\n",
    "    plt.xlabel('norm update vector')\n",
    "    plt.title('singular values')\n",
    "    plt.ylim(singular_values_ylim)\n",
    "    \n",
    "    plt.subplot(2,3,4)\n",
    "    plt.plot(vec_norms, in_subspace['erank_vals'], 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('erank')\n",
    "    plt.xlabel('norm update vector')\n",
    "    plt.title('erank')\n",
    "    plt.ylim(erank_ylim)\n",
    "    \n",
    "    plt.subplot(2,3,5)\n",
    "    plt.plot(vec_norms, in_subspace['erank_grad_norm_vals'], 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('grad norm erank')\n",
    "    plt.xlabel('norm update vector')\n",
    "    plt.title('gradient norm of concatenated vector')\n",
    "    \n",
    "    plt.subplot(2,3,6)\n",
    "    plt.plot(vec_norms, in_subspace['singular_vals'], 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('singular value')\n",
    "    plt.xlabel('norm update vector')\n",
    "    plt.title('singular values')\n",
    "    plt.ylim(singular_values_ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnormalized Direction Matrix M - axis parallel dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! erank_svd\n",
    "xlim_logscale=[-7,4]\n",
    "normalize_dir_matrix=False\n",
    "# outside subspace: axis parallel direction\n",
    "update_vec1 = torch.zeros(size=(1, dir_matrix.shape[1]), requires_grad=False)\n",
    "update_vec1[0,0] = 1.\n",
    "out_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec1, erank_fn=erank_svd, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "# inside subspace: mean vec of dir_matrix\n",
    "update_vec2 = dir_matrix.mean(dim=0, keepdim=True)\n",
    "in_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec2, erank_fn=erank_svd, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "make_plot(out_subspace, in_subspace)\n",
    "np.isnan(np.array(out_subspace['erank_grad_norm_vals'])).any(),np.isnan(np.array(in_subspace['erank_grad_norm_vals'])).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! erank_pca_lr\n",
    "xlim_logscale=[-7,4]\n",
    "normalize_dir_matrix=False\n",
    "# outside subspace: axis parallel direction\n",
    "update_vec1 = torch.zeros(size=(1, dir_matrix.shape[1]), requires_grad=False)\n",
    "update_vec1[0,0] = 1.\n",
    "out_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec1, erank_fn=erank_pca_lr, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "# inside subspace: mean vec of dir_matrix\n",
    "update_vec2 = dir_matrix.mean(dim=0, keepdim=True)\n",
    "in_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec2, erank_fn=erank_pca_lr, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "make_plot(out_subspace, in_subspace)\n",
    "np.isnan(np.array(out_subspace['erank_grad_norm_vals'])).any(),np.isnan(np.array(in_subspace['erank_grad_norm_vals'])).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Direction Matrix M - axis parallel dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim_logscale=[-7,4]\n",
    "normalize_dir_matrix=True\n",
    "# outside subspace: axis parallel direction\n",
    "update_vec1 = torch.zeros(size=(1, dir_matrix.shape[1]), requires_grad=False)\n",
    "update_vec1[0,0] = 1.\n",
    "out_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec1, erank_fn=erank_svd, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "# inside subspace: mean vec of dir_matrix\n",
    "update_vec2 = dir_matrix.mean(dim=0, keepdim=True)\n",
    "in_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec2, erank_fn=erank_svd, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "make_plot(out_subspace, in_subspace)\n",
    "np.isnan(np.array(out_subspace['erank_grad_norm_vals'])).any(),np.isnan(np.array(in_subspace['erank_grad_norm_vals'])).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnormalized M - Random direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim_logscale=[-19,4]\n",
    "normalize_dir_matrix=False\n",
    "# outside subspace: axis parallel direction\n",
    "update_vec1 = torch.normal(0, 1, size=(1, dir_matrix.shape[1]), requires_grad=False)\n",
    "out_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec1, erank_fn=erank_svd, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "# inside subspace: mean vec of dir_matrix\n",
    "update_vec2 = dir_matrix.mean(dim=0, keepdim=True)\n",
    "in_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec2, erank_fn=erank_svd, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "make_plot(out_subspace, in_subspace)\n",
    "np.isnan(np.array(out_subspace['erank_grad_norm_vals'])).any(),np.isnan(np.array(in_subspace['erank_grad_norm_vals'])).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim_logscale=[-9,4]\n",
    "normalize_dir_matrix=False\n",
    "# outside subspace: axis parallel direction\n",
    "update_vec1 = torch.normal(0, 1, size=(1, dir_matrix.shape[1]), requires_grad=False)\n",
    "out_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec1, erank_fn=erank_pca_lr, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "# inside subspace: mean vec of dir_matrix\n",
    "update_vec2 = dir_matrix.mean(dim=0, keepdim=True)\n",
    "in_subspace = erank_sv_norm_update_vector_sweep(dir_matrix, update_vec2, erank_fn=erank_pca_lr, normalize_dir_matrix=normalize_dir_matrix, xlim_logscale=xlim_logscale)\n",
    "make_plot(out_subspace, in_subspace)\n",
    "np.isnan(np.array(out_subspace['erank_grad_norm_vals'])).any(),np.isnan(np.array(in_subspace['erank_grad_norm_vals'])).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we compute erank with svd on zero vector?\n",
    "\n",
    "norm_update_vec = 0.0\n",
    "\n",
    "delta = torch.zeros(size=(1,dir_matrix.shape[1])).to(dir_matrix)\n",
    "# delta[0] = norm_update_vec\n",
    "delta.requires_grad_(True)\n",
    "matrix1 = torch.cat([delta, dir_matrix], dim=0)\n",
    "_, s, _ = torch.linalg.svd(matrix1)\n",
    "erank_val = erank_svd(matrix1)\n",
    "erank_val.backward()\n",
    "erank_grad_norm = torch.linalg.norm(delta.grad)\n",
    "s, erank_val, erank_grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook we analyzed the norm of the gradients of the erank with respect to the (concatenated) update vector. The motivation were the non-smooth behavior of the erank around the origin. We want to find out how we can perturb the parameters such that we stay in the smooth region of the erank.\n",
    "\n",
    "The erank function looks differently, depending on the direction of the input parameter (See especially the plots for the unnormalized directions matrix M):\n",
    "- If we go in directions outside the subspace, the erank function has a peak at around the lengths of the other vectors.\n",
    "- If we go in directions inside the subspace, this peak does not occur.\n",
    "\n",
    "This has some implications for the noise we want to add:\n",
    "We assume that a random Gauss vector is not inside the subspace. We add a random Normal distributed vector scaled by epsilon = 1e-6 to avoid numeric instabilities.\n",
    "\n",
    "The observations are made with a random normal distributed direction matrix (a random subspace) and a subspace spanned by pretrained models on f_mnist.\n",
    "\n",
    "### Compare: Function to compute singular values: torch.linalg.svd vs. torch.pca_lowrank\n",
    "-> use torch.linalg.svd!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('erank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1e300651644b4a5ccc8598fb3da49325ec88b18d4f380e9c939826d448ae9e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
